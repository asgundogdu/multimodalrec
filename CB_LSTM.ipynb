{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training User-Movie Latent Factors are extracting...\n",
      "The sparsity level of training dataset is 95.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 92/2930 [00:00<00:03, 917.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Visual Representations are extracting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2930/2930 [00:03<00:00, 921.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from multimodalrec.multimodalrec import MultimodalRec\n",
    "from multimodalrec.multimodalrec import data_pipeline\n",
    "from multimodalrec.model import model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "recmodel = MultimodalRec()\n",
    "recmodel.organize_multimodal_data(load=True)\n",
    "ratings_df_training = recmodel.user_item_network_training.CF_data\n",
    "ratings_df_training[\"Rating\"] = ratings_df_training[\"Rating\"]/5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1_frames = 30\n",
    "input_1_dimension = 2048\n",
    "hidden_size = 5\n",
    "input_2_dimension = 50\n",
    "batch_size = 64\n",
    "is_training = True\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope('main_parameters'):\n",
    "    x_lstm = tf.placeholder(tf.float32, [None, input_1_frames, input_1_dimension], name='Input_LSTM')\n",
    "    x_lstm_seq_length = tf.placeholder(tf.int32, [None], name='Input_LSTM_Seq_lenght')\n",
    "    x_lstm_keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    tf.summary.scalar(x_lstm.op.name, tf.reduce_mean(x_lstm))\n",
    "    tf.summary.histogram(x_lstm.op.name, tf.reduce_mean(x_lstm))\n",
    "    \n",
    "    x_fusion = tf.placeholder(tf.float32, shape=[None, input_2_dimension], name='Input_fusion')\n",
    "    tf.summary.scalar(x_fusion.op.name, tf.reduce_mean(x_fusion))\n",
    "    tf.summary.histogram(x_fusion.op.name, tf.reduce_mean(x_fusion))\n",
    "    \n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1], name='Output')\n",
    "    tf.summary.scalar(y.op.name, tf.reduce_mean(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'main_parameters/Input_LSTM:0' shape=(?, 30, 2048) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('frame_level_lstm') as scope:\n",
    "    #seq_flat = tf.reshape(x_lstm, [-1, input_2_dimension])\n",
    "    # [batch_size * max_length, hidden_size]\n",
    "    #seq_embed = tf.contrib.layers.fully_connected(seq_flat, hidden_size)\n",
    "    seq_embed = tf.reshape(x_lstm, [-1, input_1_frames, hidden_size])\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.LSTMCell(hidden_size, state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=x_lstm_keep_prob)\n",
    "    init_state = cell.zero_state(tf.shape(x_lstm)[0], dtype=tf.float32)\n",
    "\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x_lstm, sequence_length=x_lstm_seq_length,\n",
    "                                        initial_state=init_state, dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    batch_range = tf.range(tf.shape(outputs)[0])\n",
    "    last_index = x_lstm_seq_length - 1\n",
    "    indices = tf.stack([batch_range, last_index], axis=1)\n",
    "    last = tf.gather_nd(outputs, indices)\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('concat_level') as scope:\n",
    "    #user_dense = tf.layers.dense(inputs=x_fusion, units=64, activation=tf.nn.relu)\n",
    "    concat_tensor = tf.concat(axis=1,values=[last, tf.nn.l2_normalize(x_fusion)])\n",
    "    #concat_tensor = tf.concat(axis=1,values=[dense2_frames, x_fusion])\n",
    "    tf.summary.scalar('concat_tensor/bimodal', tf.reduce_mean(concat_tensor))\n",
    "    tf.summary.histogram('concat_tensor/bimodal', tf.reduce_mean(concat_tensor))\n",
    "    \n",
    "with tf.variable_scope('regression_level') as scope:\n",
    "    layer_1 = tf.layers.dense(inputs=concat_tensor, units=32, activation=tf.nn.relu)\n",
    "    tf.summary.scalar('layer_1', tf.reduce_mean(layer_1))\n",
    "    tf.summary.histogram('layer_1', tf.reduce_mean(layer_1))\n",
    "    \n",
    "    W_1 = tf.Variable(tf.random_uniform([32,1]))\n",
    "    b_1 = tf.Variable(tf.zeros([1]))\n",
    "    pred = tf.nn.sigmoid(tf.add(tf.matmul(layer_1,W_1), b_1))\n",
    "    #layer_1 = \n",
    "      \n",
    "    #pred = tf.layers.dense(inputs=layer_1, units=1, activation=tf.nn.sigmoid)\n",
    "    tf.summary.scalar('pred', tf.reduce_mean(pred))\n",
    "    tf.summary.histogram('pred', tf.reduce_mean(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name frame_level_lstm/rnn/lstm_cell/kernel:0 is illegal; using frame_level_lstm/rnn/lstm_cell/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name frame_level_lstm/rnn/lstm_cell/bias:0 is illegal; using frame_level_lstm/rnn/lstm_cell/bias_0 instead.\n",
      "INFO:tensorflow:Summary name regression_level/dense/kernel:0 is illegal; using regression_level/dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name regression_level/dense/bias:0 is illegal; using regression_level/dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name regression_level/Variable:0 is illegal; using regression_level/Variable_0 instead.\n",
      "INFO:tensorflow:Summary name regression_level/Variable_1:0 is illegal; using regression_level/Variable_1_0 instead.\n"
     ]
    }
   ],
   "source": [
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss/Sqrt_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope('loss') as scope:\n",
    "    cost = tf.sqrt(tf.reduce_mean(tf.square(y - pred)))\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.1).minimize(cost)#tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "tf.summary.scalar(cost.op.name, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./model/cb_lstm/\"\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42905\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    #print(variable)\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    #print(shape)\n",
    "    #print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        #print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    #print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_=64\n",
    "epoch_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Epoch 0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "train_writer = tf.summary.FileWriter(save_dir, sess.graph)\n",
    "\n",
    "for i in range(epoch_num):\n",
    "    print('-'*30)\n",
    "    print('Epoch {}'.format(i))\n",
    "    print('-'*30)\n",
    "    \n",
    "    # for batch in range(int(ratings_df_training.shape[0]/batch_size) + 1):\n",
    "    for batch in range(30):\n",
    "        X_train_lstm, X_train_fusion, y_train, indices = data_pipeline(ratings_df_training, recmodel.user_latent_traninig, recmodel.visual_features, \n",
    "                                                                       batch_size_, batch, epoch_num=i, random=True)  \n",
    "        #print(len(y_train[batch * batch_size_: (batch + 1) * batch_size_]))\n",
    "        batch_lstm_xs = np.array(X_train_lstm, dtype=float)\n",
    "        batch_train_xs = np.array(X_train_fusion, dtype=float)\n",
    "        batch_ys = np.array(y_train, dtype=float).reshape(batch_size_,1)\n",
    "        batch_seq_lenth_xs = np.array([2048]*batch_size)\n",
    "\n",
    "        #print(\"{} {} {} {}\".format(batch_lstm_xs.shape, batch_train_xs.shape,batch_ys.shape,batch_seq_lenth_xs.shape))\n",
    "\n",
    "\n",
    "        _, batch_loss = sess.run(\n",
    "            [optimizer, cost],\n",
    "            feed_dict={x_lstm: batch_lstm_xs,\n",
    "                       x_lstm_seq_length:batch_seq_lenth_xs,\n",
    "                       x_lstm_keep_prob:0.5,\n",
    "                       x_fusion: batch_train_xs,\n",
    "                       y: batch_ys})\n",
    "        msg = \"Batch {} - Training Batch Loss: {:.4f}\"\n",
    "        print(msg.format((batch+1), batch_loss))\n",
    "        \n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        summary, _ = sess.run([merged, optimizer],\n",
    "                              feed_dict={x_lstm: batch_lstm_xs,\n",
    "                                           x_lstm_seq_length:batch_seq_lenth_xs,\n",
    "                                           x_lstm_keep_prob:0.5,\n",
    "                                           x_fusion: batch_train_xs,\n",
    "                                           y: batch_ys},\n",
    "                              options=run_options,\n",
    "                              run_metadata=run_metadata)\n",
    "        train_writer.add_summary(summary, batch+(i*30))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #summary, _ = sess.run([merged, train_step],\n",
    "        #                      feed_dict=feed_dict(True),\n",
    "        #                      options=run_options,\n",
    "        #                      run_metadata=run_metadata)\n",
    "        train_writer.add_run_metadata(run_metadata, 'step%03d' % int(batch+(i*30)+1))\n",
    "        # train_writer.add_summary(summary, i)\n",
    "        \n",
    "            \n",
    "    X_train_lstm, X_train_fusion, y_train, indices = data_pipeline(ratings_df_training, recmodel.user_latent_traninig, recmodel.visual_features, 1000, i, validation=True)  \n",
    "    #print(len(y_train[batch * batch_size_: (batch + 1) * batch_size_]))\n",
    "    batch_lstm_xs = np.array(X_train_lstm, dtype=float)\n",
    "    batch_train_xs = np.array(X_train_fusion, dtype=float)\n",
    "    batch_ys = np.array(y_train, dtype=float).reshape(3000,1)\n",
    "    batch_seq_lenth_xs = np.array([2048]*batch_size)\n",
    "    predictions = sess.run(\n",
    "        [pred],\n",
    "        feed_dict={x_lstm: batch_lstm_xs,\n",
    "                   x_lstm_seq_length:batch_seq_lenth_xs,\n",
    "                   x_lstm_keep_prob:0.5,\n",
    "                   x_fusion: batch_train_xs,\n",
    "                   y: batch_ys})\n",
    "\n",
    "    RMSE = np.sqrt(np.mean((predictions[0]-y_train)**2))\n",
    "\n",
    "    msg = \"Epoch {} - RMSE: {}\"\n",
    "    print(msg.format((i+1), RMSE))\n",
    "    \n",
    "train_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
